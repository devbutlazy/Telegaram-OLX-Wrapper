from aiogram import Bot
from aiogram.types import URLInputFile

import asyncio
from datetime import datetime
from typing import Any
import logging

import aiohttp
from bs4 import BeautifulSoup
from database import user_tags, NEW_ITEMS_URL, SEARCH_URL, MAIN_SITE

logging.basicConfig(
    format="\033[1;32;48m[%(asctime)s] | %(levelname)s | %(message)s\033[1;37;0m",
    level=logging.INFO,
)


async def fetch(session: aiohttp.ClientSession, url: str) -> str:
    async with session.get(url) as response:
        return await response.text()


def escape_markdown(text: str) -> str:
    escape_chars = [".", "-", "(", ")", "[", "]", "|", "#"]
    return "".join(f"\\{char}" if char in escape_chars else char for char in text)


async def check_new_items(
    bot: Bot, session: aiohttp.ClientSession, tag: str, data: Any
) -> None:
    message_text = (
        f"<a href='https://pbt.storage.yandexcloud.net/cp_upload/81dfe15e19c39647389362b9781aa17f_full.png'>üìå</a> <b>–ù–æ–≤–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –ø–æ</b> #{tag}\n\n"
        f"üîó *–ü–æ—Å–∏–ª–∞–Ω–Ω—è:* https://olx.com.ua\n"
        f"üìõ *–ù–∞–∑–≤–∞:* –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è\n"
        f"üí∞ *–¶—ñ–Ω–∞:* 150–≥—Ä–Ω\n"
        f"üìä *–°—Ç–∞–Ω:* –ë/–í\n"
        f"üìç *–ú—ñ—Å—Ç–æ:* –ß–µ—Ä–Ω—ñ–≤—Ü—ñ, –≤—É–ª. –¢–∏—Ö–∞ - 17:09\n"
        f"üìû *–ü—Ä–æ–¥–∞–≤–µ—Ü—å:* –Ñ–≤–≥–µ–Ω—ñ–π | –ë—ñ–∑–Ω–µ—Å –ê–∫–∫–∞—É–Ω—Ç\n"
    )
    await bot.send_message(
        chat_id=data.get("chat_id"), text=message_text, parse_mode="html"
    )
    while True:
        result = await fetch(
            session, NEW_ITEMS_URL.format(target=tag, last_id=data.get("last_id"))
        )
        product_soup = BeautifulSoup(result, "html.parser")
        product_elements = product_soup.find_all("a", class_="css-rc5s2u")

        if data.get("last_id"):
            parsed_info = BeautifulSoup(
                await fetch(session, MAIN_SITE + product_elements[0].get("href")),
                "html.parser",
            )

            await user_tags.update_one(
                {"user_id": data.get("user_id")},
                {
                    "$set": {
                        "last_id": int(
                            parsed_info.find(
                                "span", class_="css-12hdxwj er34gjf0"
                            ).text.replace("ID: ", "")
                        )
                    }
                },
            )

            data.update(
                {
                    "last_id": int(
                        parsed_info.find(
                            "span", class_="css-12hdxwj er34gjf0"
                        ).text.replace("ID: ", "")
                    )
                }
            )

        for element in product_elements:
            print(element.find("p", class_="css-1kyngsx er34gjf0"))
            parsed_info = BeautifulSoup(
                await fetch(session, f"{MAIN_SITE}{element.get('href')}"), "html.parser"
            )
            if not element.find("p", class_="css-1kyngsx er34gjf0"):
                return
            if int(
                parsed_info.find("span", class_="css-12hdxwj er34gjf0").text.replace(
                    "ID: ", ""
                )
            ) != data.get("last_id"):
                await bot.send_message(
                    chat_id=data.get("chat_id"),
                    text=(
                        f"<a href='https://pbt.storage.yandexcloud.net/cp_upload/81dfe15e19c39647389362b9781aa17f_full.png'>üìå</a> <b>–ù–æ–≤–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –ø–æ</b> #{tag}\n\n"
                        f"üîó <b>–ü–æ—Å–∏–ª–∞–Ω–Ω—è:</b> {MAIN_SITE}{element.get('href')}\n"
                        f"üìõ <b>–ù–∞–∑–≤–∞:</b> {element.find('h6', class_='css-16v5mdi er34gjf0').text}\n"
                        f"üí∞ <b>–¶—ñ–Ω–∞:</b> {parsed_info.find('h3', class_='css-12vqlj3').text if parsed_info.find('h3', class_='css-12vqlj3') else '–ù–µ –≤–∫–∞–∑–∞–Ω–∞'}\n"
                        f"üìä <b>–°—Ç–∞–Ω:</b> {element.find('span', class_='css-3lkihg').text if element.find('span', class_='css-3lkihg') else '–ù–µ –≤–∫–∞–∑–∞–Ω–æ'}\n"
                        f"üìç <b>–ú—ñ—Å—Ç–æ:</b> {element.find('p', class_='css-1a4brun er34gjf0').text}\n"
                        f"üìû <b>–ü—Ä–æ–¥–∞–≤–µ—Ü—å:</b> {parsed_info.find('h4', class_='css-1lcz6o7 er34gjf0').text} | {parsed_info.find('p', class_='css-b5m1rv er34gjf0').text}\n"
                    ),
                    parse_mode="html",
                )

                # await user_tags.update_one(
                #     {"user_id": data.get("user_id")},
                #     {
                #         "$set": {
                #             "last_id": (
                #                 int(
                #                     parsed_info.find(
                #                         "span", class_="css-12hdxwj er34gjf0"
                #                     ).text.replace("ID: ", "")
                #                 )
                #             )
                #         }
                #     },
                #     upsert=True,
                # )

        else:
            logging.info(f"[{datetime.now().strftime('%H:%M:%S')}] No new items found")
            await asyncio.sleep(300)


async def scrape_info(session: aiohttp.ClientSession, element: Any, data: Any) -> None:
    """
    Scrape OLX website for listings related to the target item.

    Params:
    - element: Any - elements to search from
    """
    if element.get("href") == "https://":
        return

    parser = BeautifulSoup(
        await fetch(session, f"{MAIN_SITE}{element.get('href')}"), "html.parser"
    )

    await user_tags.update_one(
        {"user_id": data.get("user_id")},
        {
            "$set": {
                "last_id": int(
                    parser.find("span", class_="css-12hdxwj er34gjf0").text.replace(
                        "ID: ", ""
                    )
                ),
            }
        },
        upsert=True,
    )


async def process_tags(bot: Bot, session, data):
    for tag in data.get("tags"):
        site = await fetch(session, SEARCH_URL.format(target=tag))
        product_soup = BeautifulSoup(site, "html.parser")
        product_elements = product_soup.find_all("a", class_="css-rc5s2u")

        tasks = [scrape_info(session, element, data) for element in product_elements]
        await asyncio.gather(*tasks)
        await check_new_items(bot, session, tag=tag, data=data)


async def main(bot: Bot):
    logger = logging.getLogger("aiogram")
    logger.info("Successfully scraped all items")
    logger.info("Started loop for checking new items")

    async for data in user_tags.find():
        async with aiohttp.ClientSession() as session:
            tags = data.get("tags")

            if tags is None:
                continue

            if isinstance(tags, int):
                tags = [tags]

            await process_tags(bot, session, data)
